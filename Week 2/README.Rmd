---
title: '`Week 2` Regression Models'
author: '`r if(knitr::is_html_output()) {"&#x1f468;&#x1F3FB;&#x200d;&#x1f4bb; Anderson H Uyekita"} else {NULL}`'
output:
  github_document: default
params:
  author: 'Anderson H Uyekita'
  course: 'Regression Models'
  course_url: 'https://www.coursera.org/learn/regression-models'
  specialization: 'Data Science: Statistics and Machine Learning Specialization'
  specialization_url: 'https://www.coursera.org/specializations/data-science-statistics-machine-learning'
  instructor: 'Brian Caffo'
  course_start: !r lubridate::ymd('2022/07/05', tz = 'America/Sao_Paulo')
  course_finish: !r lubridate::ymd('2022/07/18', tz = 'America/Sao_Paulo')
  week: ' Week 2'
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, message = FALSE, warning = FALSE)
library(rmarkdown)
library(lubridate)
library(UsingR)
library(ggplot2)
library(tidyverse)
```

`r if(!knitr::is_html_output()) {sprintf(fmt = "* &#x1f468;&#x1F3FB;&#x200d;&#x1f4bb; Author: %s", params$author)}`
`r sprintf(fmt = "* &#x1f4da; Specialization: [%s](%s){target='_blank' rel='noopener'}", params$specialization, params$specialization_url)`
`r sprintf(fmt = "* &#x1f4d6; Course: [%s](%s){target='_blank' rel='noopener'}", params$course, params$course_url)`
    `r sprintf(fmt = "* &#x1F9D1;&#x200d;&#x1F3EB; Instructor: %s", params$instructor)`
`r sprintf(fmt = "* &#x1F4C6; %s", params$week)`
    `r sprintf(fmt = "* &#x1F6A6; Start: %s", format(params$course_start, "%A, %d %B %Y"))`
    `r sprintf(fmt = "* &#x1F3C1; Finish: %s", format(params$course_finish, "%A, %d %B %Y"))`

--------------------------------------------------------------------------------

#### Assignments & Deliverables

* [&#x1F680; Course Project 2 Repository](https://github.com/AndersonUyekita/regression-models_course-project-2)
* [&#x1F4DD; Quiz 2](./quiz-2_regression-models.md)

#### Slides

* Module 2 -- Linear Regression & Multivariable Regression
    * 02_01 Multivariate regression
    * 02_02 Multivariate examples
    * 02_03 Adjustment
    * 02_04 Residual variation and diagnostics
    * 02_05 Multiple variables

#### Description

> This week, we will work through the remainder of linear regression and then turn to the first part of multivariable regression.

--------------------------------------------------------------------------------

## Class Notes

#### Statistical linear regression models

> Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know **how to extend our estimates to a population**. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity.  

For this lecture we are going to use the `diamond` dataset from `UsingR` package.

##### Case 1 -- Without Centering Data

```{r}
# Loading Diamond dataset in Environment.
data("diamond")

# Printing the structure
str(diamond)
```

Now that we know the variables' names let's create a linear regression to explain the price (in SIN units -- Singapore Dollar) by the diamond carats.

```{r}
# Calculating the linear regression with intercept.
#
# lm(formula = 'What we want to explain' ~ 'What we think is a good estimator' )
#
# If we do not want intercept just add -1
#
# lm(formula = 'What we want to explain' ~ 'What we think is a good estimator' - 1 ) # With no intercept
#
fit <- lm(data = diamond, formula = price ~ carat)

# Printing results.
fit$coefficients
```
**Explanation/Interpretation**

> We estimate an expected 3721.02 SIN dollar increase in price for every carat increase in mass of diamond, and.
> The intercept negative 259.63 means the expected price of a 0 carat diamond is 259.63 (!!!!).

**WARNING**

Although we have **Rejected** $H_0:\beta = 0$, it is hard to believe in paying 259.63 SIN for a diamond with 0 carats. It is not possible to explain it.

##### Case 2 -- Centering Data

At this time, we are "centering" the dataset on creating an interpretable model. We only need to center the carat variable.

```{r}
# We do not have problem with price variable.
price <- diamond$price

# Centering carat variable.
carat_c <- diamond$carat - mean(diamond$carat)

# Fitting a new linear model
fit2 <- lm(formula = price ~ carat_c, data = data.frame("prince" = price, "carat_c" = carat_c))

# Printing the coefficients
summary(fit2)$coeff
```

We have centered the carat; the Intercept has changed to a positive number.

**Explanation/Interpretation**

> * Notice the estimated slope didn’t change at all.
> * Thus 500.1 SIN is the expected price for the average sized diamond of the data (0.2042 carats).

##### Predicting based on a fitted model

**Using model `fit`**

```{r}
# Sample of 3 diamonds with x carats.
newx <- c(0.16, 0.27, 0.34)

# Calculating using predict.
predict(fit, newdata = data.frame(carat = newx))
```

**Using model `fit2`**

```{r}
# Centering the carat new sample.
newx2 <- c(0.16, 0.27, 0.34) - mean(diamond$carat)

# Calculating "manually".
fit2$coefficients[1] + fit2$coefficients[2]*newx2
```

Now using `predict()`.

```{r}
predict(fit2, newdata = data.frame(carat_c = newx2))
```

It is important to keep the `newdata` column name in accordance with the `data` used in `lm()`.

#### Residuals

> **Residuals represent variation left unexplained by our model.** We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors.  

My model:

$$Y_i = \beta_O + \beta_1 \cdot X_i + \epsilon_i$$

Where:

* $\epsilon_i$ is the error in $i$ observation.

Assuming the notation $\hat Y_i$ to the predicted value to $i$ observation.

$$\epsilon_i = Y_i - \hat Y_i$$

So $\epsilon_i \thicksim N(0,\sigma^2)$.

##### Example calculating residual

Let's use the `diamond` dataset.

```{r}
# loading diamond dataset.
data(diamond)

# Assing prince to y; carat to x and creating variable n as number of observations.
y <- diamond$price; x <- diamond$carat; n <- length(y)

# Fitting a model with intercept.
fit <- lm(y ~ x)
```

The residual is calculated using `resid()`.

```{r}
## The easiest way to get the residuals
e <- resid(fit)

# Printing the residual
e
```

Or manually calculating:

```{r}
## Obtain the residuals manually, get the predicted Ys first
yhat <- predict(fit)

## Printing the residual
y - yhat
```

There is "no" difference using built-in function `redis()` or manually.

```{r}
# Comparisson
e - (y - yhat)
```

**There are a small difference.**

##### Plotting the residual

Using the residual calculated in the diamond dataset.

```{r}
# Plotting a ggplot
ggplot(data = diamond, aes(x = carat, y = e)) + 
    geom_point() + 
    xlab("Carat") + 
    ylab("Residual (SIN)")
```

For the plot above, there may be no trend or pattern. Therefore, we classify the residual as a homogeneous spread on the graph, the so-called homoscedastic.

##### Heterocedastic Residual

Figure 1 shows a heteroscedastic residual, meaning the fit model is not designed correctly.

![](./figures/figure_1.png)

In the Figure 2 case, the residual has a sinusoidal format, which we could understand that we have modeled only the linear part and left valuable info in the residual. So the residuals are not white noise and must be adjusted to extract this sinusoidal part.

![](./figures/figure_2.png)

##### Residual Variance

Residual Variance is the $\sigma^2$.

```{r}
# Modeling
fit <- lm(data = diamond, price ~ carat)

# Printing the sigma from summary
summary(fit)$sigma
```

It is also possible calculate it manually.

```{r}
# For small sample the -2 make difference.
sqrt(sum(resid(fit)^2)/(length(diamond$carat) - 2))
```

##### R2

From the total variability in price (`resid(lm(data = diamond, price ~ 1))`), how much residual variability I can explain when I use one variable to explain it, in this case, carat (`resid(lm(data = diamond, price ~ carat))`).

Manually the $R^2$ is calculated:

```{r}
# Calculating the total variability.
e_total <- resid(lm(data = diamond, price ~ 1))

# Calculating the residual variability after fitting
e_lm <- resid(lm(data = diamond, price ~ carat))

# Calculating the R2
(sum(e_total^2) - sum(e_lm^2))/sum(e_total^2)
```

Calculating using the `summary()`.

```{r}
# Printing the R2.
summary(lm(data = diamond, price ~ carat))$r.squared
```

$$\text{Total variability} = \text{Residual variability} + \text{Regression variability}$$

Where

$$\text{Regression variability} = \sum_{i=1}^{n}{\Big( Y_i - \hat Y_i \Big)^2}$$
```{r}
# Calculating the Regression variability
sum(e_total^2) - sum(e_lm^2)
```


#### Inference in regression

> Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference. 
>
> These statements apply generally, and, of course, to the regression setting that we've been studying. In the next few lectures, we'll cover inference in regression where we make some Gaussian assumptions about the errors.

According to:

$$\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}}$$

We have the properties:

>* They are normally distributed and have a finite sample Student’s T distribution under normality assumptions.
>* They can be used to test:
>    * $H_0 : \theta = \theta_0$ and $H_a : \theta >,\neq,< \theta_0$
>* They can be used to create a confidence interval $\theta$ via $\hat \theta \pm Q_{1-\frac{\alpha}{2}} \cdot \hat \sigma_{\hat \theta}$ where $Q_{1-\frac{\alpha}{2}}$ is the relevant quantile from either a normal or T distribution.

Let's calculate the fit model.

```{r}
# calculating the fit model.
fit <- lm(data = diamond, price ~ carat)

# Calculating the summary
fit_summary <- summary(fit)

# Printing the coefficients.
fit_summary$coefficients
```

> Remember, we reject if our P-value is less than our desired type I error rate.

For this example, the p-value is very low (less then $\alpha$), so we **Reject** $H_0 : \beta_1 = 0$, which means there is a linear relation. However, if we **Failed to Reject** $H_O : \beta_1 = 0$, so there is no linear relation, because the **slope is zero**.

Concerning the intercept, there is the same understanding. If we Reject the $H_1 : \beta_0 = 0$, there is an intercept different from zero. Although, if we **Failed to Reject** $H_0 : \beta_0 = 0$, the intercept is zero.

Now we have all values it is possible to calculate the Confidence Interval.

$$\text{Intercept} \pm quantile_{1-\frac{\alpha}{2}} \cdot \sigma_{error}$$
Calculating in R.

```{r}
# Intercept
fit_summary$coefficients[1,1] + c(-1, 1) * qt(p = (1 - 0.05/2), df = fit$df) * fit_summary$coefficients[1,2];

# Slope
fit_summary$coefficients[2,1] + c(-1, 1) * qt(p = (1 - 0.05/2), df = fit$df) * fit_summary$coefficients[2,2]
```

>So, we would interpret this as: “with 95% confidence, we estimate that a 1 carat increase in diamond size results in a 3556.4 to 3885.6 increase in price in (Singapore) dollars”

##### Prediction

```{r}
library(ggplot2)

fit <- lm(formula = price ~ carat, data = diamond)

newx <- data.frame("carat" = seq(min(x), max(x), length = 100))
p1 <- data.frame(predict(fit, newdata = newx, interval = ("confidence")))
p2 <- data.frame(predict(fit, newdata = newx, interval = ("prediction")))
p1$interval <- "confidence"
p2$interval <- "prediction"
p1$x <- newx$carat
p2$x <- newx$carat
dat <- rbind(p1, p2)
names(dat)[1] = "y"

g <- ggplot(dat, aes(x = x, y = y))
g <- g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
g <- g + geom_line()
g <- g + geom_point(data = data.frame(x = x, y=y), aes(x = x, y = y), size = 4)
g
```
